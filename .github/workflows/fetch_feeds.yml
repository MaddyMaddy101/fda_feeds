name: Fetch, Scrape, and Update Wiki

on:
  schedule:
    - cron: '0 */6 * * *'  # Runs every 6 hours
  workflow_dispatch:  # Allows manual execution

jobs:
  fetch-feeds:
    runs-on: ubuntu-latest

      - name: Set Up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Dependencies
        run: |
          pip install feedparser
          pip install beautifulsoup4
          pip install requests

      - name: Run Feed Fetcher and Scraper
        run: python fetch_feeds.py
        
      - name: Commit and Push filtered_feeds.json
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add filtered_feeds.json
          git commit -m "Update filtered_feeds.json"
          git push https://github-actions:${{ secrets.GH_PAT }}@github.com/Tundrawenwen/fda_feeds.git

      - name: Debug Workspace Files
        run: ls -R  # Debug: List all files in the workspace

      - name: Checkout Wiki Repository
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          git clone https://github-actions:${{ secrets.GH_PAT }}@github.com/${{ github.repository }}.wiki.git wiki-repo

      - name: Copy Updated Wiki File
        run: |
          cp Filtered-Feeds.md wiki-repo/

      - name: Commit and Push Changes to Wiki
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          cd wiki-repo
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add Filtered-Feeds.md
          git commit -m "Updated filtered feeds in Wiki" || echo "No changes to commit"
          git push https://github-actions:${{ secrets.GH_PAT }}@github.com/${{ github.repository }}.wiki.git
      - name: List files in the working directory
        run: ls -al
